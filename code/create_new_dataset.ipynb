{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_new_dataset.ipynb\n",
    "\n",
    "This notebook implements my workflow for fine tuning a YOLOv8 object detection model which\n",
    "detects coconut rhinoceros beetle damage in coconut palms.\n",
    "\n",
    "# Installation\n",
    "\n",
    "Clone the repo\n",
    "```\n",
    "git clone https://github.com/aubreymoore/CRB-Damage-Dataset-Improvement\n",
    "```\n",
    "\n",
    "Move to the new folder\n",
    "```\n",
    "cd CRB-Damage-Dataset-Improvement\n",
    "```\n",
    "\n",
    "Create a virtual environment\n",
    "```\n",
    "python3 -m venv .venv\n",
    "```\n",
    "\n",
    "Activate the new virtual environment\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "Install required python modules\n",
    "```\n",
    "pip install -r code/requirements.txt\n",
    "```\n",
    "\n",
    "Create a .gitignore file and add .venv to the list of files and folders to be ignored.\n",
    "Adding a virtual environmant to a repository is bad practice.\n",
    "```\n",
    "echo \".venv\" >> .gitignore\n",
    "```\n",
    "\n",
    "# References\n",
    "\n",
    "https://pybit.es/articles/a-better-place-to-put-your-python-virtual-environments/\n",
    "\n",
    "[Image Deduplication](https://github.com/voxel51/fiftyone-examples/blob/master/examples/image_deduplication.ipynb)\n",
    "\n",
    "[CVAT <> FiftyOne: Data-Centric Machine Learning with Two Open Source Tools](https://www.cvat.ai/post/data-centric)\n",
    "\n",
    "[FiftyOne - Ultralytics Integration](https://docs.voxel51.com/integrations/ultralytics.html)\n",
    "\n",
    "[Finding Detection Mistakes with FiftyOne](https://docs.voxel51.com/tutorials/detection_mistakes.html)\n",
    "\n",
    "[Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne](https://docs.voxel51.com/tutorials/yolov8.html)\n",
    "\n",
    "[FiftyOne Brain](https://docs.voxel51.com/brain.html)\n",
    "\n",
    "[Tracking Datasets in FiftyOne](https://voxel51.com/blog/tracking-datasets-in-fiftyone/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "import logging\n",
    "import sys\n",
    "from icecream import ic\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from ultralytics import YOLO\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timestamp_field():\n",
    "    dataset.add_sample_field(\"timestamp\", fo.DateTimeField)\n",
    "\n",
    "    for sample in dataset:\n",
    "        timestamp_str = os.path.basename(sample.filepath)[4:-4]\n",
    "        dt = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')\n",
    "        # ic(timestamp_str, dt)\n",
    "        sample['timestamp'] = dt\n",
    "        sample.save()\n",
    "    \n",
    "    # Create view  \n",
    "    view = dataset.sort_by(F'timestamp')\n",
    "    dataset.save_view('sorted_by_timestamp', view, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_requirements_file():\n",
    "    os.system('pip list --format=freeze > requirements.txt')\n",
    "\n",
    "# update_requirements_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    os.mkdir(NEW_DS_PATH)\n",
    "    os.mkdir(f'{NEW_DS_PATH}/images')\n",
    "    os.mkdir(f'{NEW_DS_PATH}/images/train')\n",
    "    os.mkdir(f'{NEW_DS_PATH}/images/val')\n",
    "    os.mkdir(f'{NEW_DS_PATH}/labels')\n",
    "    os.mkdir(f'{NEW_DS_PATH}/labels/train')\n",
    "    os.mkdir(f'{NEW_DS_PATH}/labels/val')\n",
    "    \n",
    "    for filepath in glob.glob(f'{ORIGINAL_DS_PATH}/train/*.jpg'):\n",
    "        shutil.copy2(filepath, f'{NEW_DS_PATH}/images/train')\n",
    "    for filepath in glob.glob(f'{ORIGINAL_DS_PATH}/train/*.txt'):\n",
    "        shutil.copy2(filepath, f'{NEW_DS_PATH}/labels/train')\n",
    "    for filepath in glob.glob(f'{ORIGINAL_DS_PATH}/val/*.jpg'):\n",
    "        shutil.copy2(filepath, f'{NEW_DS_PATH}/images/val')\n",
    "    for filepath in glob.glob(f'{ORIGINAL_DS_PATH}/val/*.txt'):\n",
    "        shutil.copy2(filepath, f'{NEW_DS_PATH}/labels/val')\n",
    "        \n",
    "    s = f'path: {NEW_DS_PATH} \\n'\n",
    "    s += 'train: ./images/train/ \\n'\n",
    "    s += 'val: ./images/val/ \\n'\n",
    "    s += 'names: \\n'\n",
    "    s += '  0: live \\n'\n",
    "    s += '  1: dead \\n'\n",
    "    s += '  2: vcut \\n'\n",
    "    with open(f'{NEW_DS_PATH}/dataset.yaml', 'w') as f:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo2fiftyone(FO_DATASET_NAME, dataset_dir, splits=[\"train\", \"val\"]):\n",
    "    \"\"\" \n",
    "    Imports a dataset in YOLO5 format into FiftyOne, using tags to mark the samples in each split \n",
    "    \"\"\" \n",
    "    dataset = fo.Dataset(name, persistent=True)\n",
    "    for split in splits:\n",
    "        dataset.add_dir(\n",
    "            dataset_dir=dataset_dir,\n",
    "            dataset_type=fo.types.YOLOv5Dataset,\n",
    "            split=split,\n",
    "            tags=split,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings_field():\n",
    "    \"\"\" \n",
    "    \"\"\" \n",
    "    model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
    "    dataset.compute_embeddings(model=model, embeddings_field='embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b)/(norm(a)*norm(b))\n",
    " \n",
    "# a = np.array([2,1,2,3,2,9])\n",
    "# b = np.array([3,4,2,4,5,5])\n",
    "# cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_similarity_with_prev_img_field():\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    view = dataset.load_saved_view('sorted_by_timestamp')\n",
    "    # thresh = 0.92\n",
    "    first_sample = True\n",
    "    for sample in view:\n",
    "        if first_sample:\n",
    "            current_embeddings = sample.embeddings\n",
    "            similarity = 0.0\n",
    "            first_sample = False\n",
    "        else:\n",
    "            previous_embeddings = current_embeddings\n",
    "            current_embeddings = sample.embeddings\n",
    "            similarity = cosine_similarity(previous_embeddings, current_embeddings)\n",
    "        sample['similarity_with_prev_img'] = similarity\n",
    "        # if similarity > thresh:\n",
    "        #     sample.tags.append(f'similarity>{thresh}')\n",
    "        # else:\n",
    "        #     sample.tags.append('similarity OK') \n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions_field():\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Load YOLOv8 model\n",
    "    # from ultralytics import YOLO\n",
    "    model = YOLO(ORIGINAL_MODEL_PATH)\n",
    "    dataset.apply_model(model, label_field=\"yolov8\")\n",
    "    \n",
    "# add_predictions_field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mistakenness_field():\n",
    "    \"\"\" \n",
    "    Adds mistakenness, possible_missing and possible_spurious fields.\n",
    "    See docs at https://docs.voxel51.com/brain.html#label-mistakes for details.\n",
    "    \"\"\"\n",
    "    fob.compute_mistakenness(dataset, \"yolov8\", label_field=\"ground_truth\")  \n",
    "    \n",
    "# add_mistakenness_field() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_field(fieldname, func):\n",
    "    \"\"\" \n",
    "    This utility function checks for existence of a field in a dataset.\n",
    "    If the field does not exist it is added by running func.\n",
    "    \"\"\"\n",
    "    if dataset.get_field(fieldname):\n",
    "        logger.info(f'\"{fieldname}\" field already exists')\n",
    "    else:\n",
    "        logger.info(f'Adding \"{fieldname}\" field')\n",
    "        func()\n",
    "\n",
    "# def add_new_field():\n",
    "#     \"\"\" \n",
    "#     Code for adding a field named 'new' should be inserted in this function.\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "    \n",
    "# add_field('new', add_new_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autocorrelated_images_view(threshold, delete=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    dataset = fo.load_dataset(FO_DATASET_NAME)\n",
    "    sorted_by_timestamp_view = dataset.load_saved_view('sorted_by_timestamp')\n",
    "    view = sorted_by_timestamp_view.match(\n",
    "        F('similarity_with_prev_img') > threshold)\n",
    "    dataset.save_view(\"autocorrelated_images_view\", view, overwrite=True)\n",
    "    count = view.count()\n",
    "    \n",
    "    if delete:\n",
    "        dataset.delete_samples(view) \n",
    "        dataset.save()\n",
    "     \n",
    "    return count\n",
    "  \n",
    "# create_autocorrelated_images_view(0.98, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ground_truth_bbs(dataset):\n",
    "    total_detections = 0\n",
    "    for sample in dataset:\n",
    "        total_detections += len(sample.ground_truth.detections)\n",
    "    return total_detections\n",
    "\n",
    "# count_ground_truth_bbs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bb_touching_edge_view(delete=False):\n",
    "    \"\"\" \n",
    "    https://docs.voxel51.com/recipes/remove_duplicate_annos.html\n",
    "    \"\"\"\n",
    "    dataset = fo.load_dataset(FO_DATASET_NAME)\n",
    "    view = dataset.filter_labels('ground_truth', \n",
    "        (F('bounding_box')[0] <= 0) | # left\n",
    "        (F('bounding_box')[1] <= 0) | # top\n",
    "        ((F('bounding_box')[0] + F('bounding_box')[3]) >= 1) # right\n",
    "    )\n",
    "    dataset.save_view('bb_touching_edge', view, overwrite=True) \n",
    "    count = view.count()\n",
    "           \n",
    "    if delete:\n",
    "        dataset.delete_labels(view)\n",
    "    dataset.save()\n",
    "            \n",
    "    return  count\n",
    "\n",
    "# create_bb_touching_edge_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unannotated_images(yolo5_dataset_path: str) -> int:\n",
    "    \"\"\" \n",
    "    Removes unannoted images from a YOLO5 data set\n",
    "    Arguments:\n",
    "        yolo5_dataset_path -- absolute path for the YOLO5 dataset\n",
    "    Returns:\n",
    "        count -- number of image (*.jpg) and annotation file pairs removed\n",
    "    \"\"\" \n",
    "    search_str = f'{yolo5_dataset_path}/**/*.txt'\n",
    "    txt_paths = glob.glob(search_str, recursive=True)\n",
    "    count = 0\n",
    "    for txt_path in txt_paths:\n",
    "        if os.path.getsize(txt_path) == 0:\n",
    "            img_path = txt_path.replace('labels', 'images').replace('.txt', '.jpg')\n",
    "            os.remove(txt_path)\n",
    "            os.remove(img_path)\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# remove_unannotated_images(\n",
    "#     yolo5_dataset_path='/home/aubrey/myexport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_51_to_YOLO(dataset_name: str, \n",
    "                      export_dir: str, \n",
    "                      remove_unannotated: bool) -> int:\n",
    "    \"\"\"\n",
    "    Export a dataset from 51 format to YOLO5 format.\n",
    "    Optionally, unannotated images will be removed from the export_dir.\n",
    "    \n",
    "    Arguments:\n",
    "    dataset_name -- a saved (persistent) 51 dataset\n",
    "    export_dir -- absolute destination path for the YOLO5 dataset\n",
    "    remove_unannoted -- if True, unannoted images are removed from the new YOLO5 dataset\n",
    "\n",
    "    Reference https://docs.voxel51.com/user_guide/export_datasets.html#yolov5dataset\n",
    "    \"\"\"\n",
    "    label_field = \"ground_truth\"\n",
    "\n",
    "    # The splits to export\n",
    "    splits = [\"train\", \"val\"]\n",
    "\n",
    "    # All splits must use the same classes list\n",
    "    classes = [\"live\", \"dead\", \"vcut\"]\n",
    "\n",
    "    # The dataset or view to export\n",
    "    # We assume the dataset uses sample tags to encode the splits to export\n",
    "    dataset_or_view = fo.load_dataset(dataset_name)\n",
    "\n",
    "    # Export the splits\n",
    "    for split in splits:\n",
    "        split_view = dataset_or_view.match_tags(split)\n",
    "        split_view.export(\n",
    "            export_dir=export_dir,\n",
    "            dataset_type=fo.types.YOLOv5Dataset,\n",
    "            label_field=label_field,\n",
    "            split=split,\n",
    "            classes=classes,\n",
    "        )\n",
    "        \n",
    "    # Remove unannotated images (optional)\n",
    "    images_removed = 0\n",
    "    if remove_unannotated:\n",
    "        images_removed = remove_unannotated_images(\n",
    "            yolo5_dataset_path=export_dir)\n",
    "    return images_removed     \n",
    "\n",
    "# export_51_to_YOLO(\n",
    "#     dataset_name='Guam07v3', \n",
    "#     export_dir='/home/aubrey/myexport', \n",
    "#     remove_unannotated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "\n",
    "    model = YOLO('/home/aubrey/label-studio-ml-backend/runs/detect/newt/weights/best.pt')\n",
    "    results = model.train(\n",
    "        resume = True,\n",
    "        imgsz=1920,\n",
    "        rect=True,\n",
    "        # data= '/home/aubrey/myexport/dataset.yaml',\n",
    "        epochs=5,\n",
    "        batch=-1,\n",
    "        patience=5,\n",
    "        name='newt'\n",
    "    )\n",
    "\n",
    "# train_model()\n",
    " \n",
    "# train model\n",
    "# !yolo \\\n",
    "# task=detect \\\n",
    "# mode=train \\\n",
    "# model= /home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks/runs/detect/train5/weights/best.pt \\\n",
    "# imgsz=1920 \\\n",
    "# data= /home/aubrey/myexport/dataset.yaml \\\n",
    "# epochs=1000 \\\n",
    "# batch=-1 \\\n",
    "# patience=50 \\\n",
    "# name=dataset3_yolov8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_cvat(anno_key_suffix: str, view) -> str:\n",
    "    \"\"\" \n",
    "    Saves a FiftyOne view in CVAT and launches the CVAT annotator\n",
    "    \n",
    "    Arguments:\n",
    "    anno_key_suffix - string     \n",
    "    view - the view to be imported into CVAT\n",
    "    \n",
    "    Result:\n",
    "    \n",
    "    anno_key - a unique string in the form of myview-2024-11-27-16:57\n",
    "    \"\"\"\n",
    "    timestamp = datetime.strftime(datetime.now(), '%Y%m%d%H%M')\n",
    "    anno_key = f'{anno_key_suffix}_{timestamp}'\n",
    "    view.annotate(\n",
    "        anno_key= anno_key,\n",
    "        label_field=\"ground_truth\", \n",
    "        launch_editor=True\n",
    "    )\n",
    "    return anno_key\n",
    "    \n",
    "# random_dozen_view = dataset.take(12)\n",
    "# launch_cvat('random_dozen', random_dozen_view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logger(LOGFILE):\n",
    "    \"\"\"\n",
    "    Configure logger to send messages to notebook and LOGFILE\n",
    "    \"\"\"\n",
    "    logging.root.handlers = []\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[\n",
    "            logging.FileHandler(filename=LOGFILE),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if NEW_DS_PATH exists:\n",
    "    continue\n",
    "else:\n",
    "    create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH)\n",
    "\n",
    "if FO_DATASET_NAME exists:\n",
    "    continue\n",
    "else:\n",
    "    yolo2fiftyone(name=FO_DATASET_NAME, dataset_dir=NEW_DS_PATH)\n",
    "    \n",
    "dataset = fo.load_dataset(FO_DATASET_NAME)\n",
    "\n",
    "# Add sample fields if they don't already exist\n",
    "add_field('timestamp', add_timestamp_field)\n",
    "add_field('embeddings', add_embeddings_field)\n",
    "add_field('similarity_with_prev_img', add_similarity_with_prev_img_field)\n",
    "add_field('yolov8', add_predictions_field)\n",
    "add_field('mistakenness', add_mistakenness_field)\n",
    "\n",
    "if 'bb_touching_edge' in dataset.list_saved_views():\n",
    "    continue\n",
    "else:\n",
    "    create_bb_touching_edge_view(DELETE_BBS_TOUCHING_EDGES)\n",
    "\n",
    "if 'autocorrelated_images_view' in dataset.list_saved_views():\n",
    "    continue\n",
    "else:\n",
    "    create_autocorrelated_images_view(AUTOCORRELATED_IMAGES_THRESHOLD, DELETE_AUTOCORRELATED_IMAGES)\n",
    "\n",
    "if RETRAIN_MODEL:\n",
    "    export_51_to_YOLO()\n",
    "    train_model()\n",
    "else:\n",
    "    continue\n",
    "\n",
    "if LAUNCH_51:\n",
    "    dataset = fo.load_dataset(FO_DATASET_NAME)\n",
    "    session = fo.launch_app(dataset, auto=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-27 17:41:07 {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob\\nimport mysecrets\\n# import ipywidgets as widgets\\n# from IPython.display import display', 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob', 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob', 'def add_timestamp_field():\\n    dataset.add_sample_field(\"timestamp\", fo.DateTimeField)\\n\\n    for sample in dataset:\\n        timestamp_str = os.path.basename(sample.filepath)[4:-4]\\n        dt = datetime.strptime(timestamp_str, \\'%Y%m%d_%H%M%S\\')\\n        # ic(timestamp_str, dt)\\n        sample[\\'timestamp\\'] = dt\\n        sample.save()\\n    \\n    # Create view  \\n    view = dataset.sort_by(F\\'timestamp\\')\\n    dataset.save_view(\\'sorted_by_timestamp\\', view, overwrite=True)', \"def update_requirements_file():\\n    os.system('pip list --format=freeze > requirements.txt')\\n\\n# update_requirements_file()\", 'def create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH):\\n    \"\"\" \\n    \"\"\"\\n    os.mkdir(NEW_DS_PATH)\\n    os.mkdir(f\\'{NEW_DS_PATH}/images\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/images/train\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/images/val\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels/train\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels/val\\')\\n    \\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/train/*.jpg\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/images/train\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/train/*.txt\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/labels/train\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/val/*.jpg\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/images/val\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/val/*.txt\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/labels/val\\')\\n        \\n    s = f\\'path: {NEW_DS_PATH} \\\\n\\'\\n    s += \\'train: ./images/train/ \\\\n\\'\\n    s += \\'val: ./images/val/ \\\\n\\'\\n    s += \\'names: \\\\n\\'\\n    s += \\'  0: live \\\\n\\'\\n    s += \\'  1: dead \\\\n\\'\\n    s += \\'  2: vcut \\\\n\\'\\n    with open(f\\'{NEW_DS_PATH}/dataset.yaml\\', \\'w\\') as f:\\n        f.write(s)', 'def yolo2fiftyone(FO_DATASET_NAME, dataset_dir, splits=[\"train\", \"val\"]):\\n    \"\"\" \\n    Imports a dataset in YOLO5 format into FiftyOne, using tags to mark the samples in each split \\n    \"\"\" \\n    dataset = fo.Dataset(name, persistent=True)\\n    for split in splits:\\n        dataset.add_dir(\\n            dataset_dir=dataset_dir,\\n            dataset_type=fo.types.YOLOv5Dataset,\\n            split=split,\\n            tags=split,\\n    )\\n    return dataset', 'def add_embeddings_field():\\n    \"\"\" \\n    \"\"\" \\n    model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\\n    dataset.compute_embeddings(model=model, embeddings_field=\\'embeddings\\')', 'def cosine_similarity(a, b):\\n    return np.dot(a,b)/(norm(a)*norm(b))\\n \\n# a = np.array([2,1,2,3,2,9])\\n# b = np.array([3,4,2,4,5,5])\\n# cosine_similarity(a, b)', 'def add_similarity_with_prev_img_field():\\n    \"\"\" \\n    \"\"\"\\n    view = dataset.load_saved_view(\\'sorted_by_timestamp\\')\\n    # thresh = 0.92\\n    first_sample = True\\n    for sample in view:\\n        if first_sample:\\n            current_embeddings = sample.embeddings\\n            similarity = 0.0\\n            first_sample = False\\n        else:\\n            previous_embeddings = current_embeddings\\n            current_embeddings = sample.embeddings\\n            similarity = cosine_similarity(previous_embeddings, current_embeddings)\\n        sample[\\'similarity_with_prev_img\\'] = similarity\\n        # if similarity > thresh:\\n        #     sample.tags.append(f\\'similarity>{thresh}\\')\\n        # else:\\n        #     sample.tags.append(\\'similarity OK\\') \\n        sample.save()', 'def add_predictions_field():\\n    \"\"\" \\n    \"\"\"\\n    # Load YOLOv8 model\\n    # from ultralytics import YOLO\\n    model = YOLO(ORIGINAL_MODEL_PATH)\\n    dataset.apply_model(model, label_field=\"yolov8\")\\n    \\n# add_predictions_field()', 'def add_mistakenness_field():\\n    \"\"\" \\n    Adds mistakenness, possible_missing and possible_spurious fields.\\n    See docs at https://docs.voxel51.com/brain.html#label-mistakes for details.\\n    \"\"\"\\n    fob.compute_mistakenness(dataset, \"yolov8\", label_field=\"ground_truth\")  \\n    \\n# add_mistakenness_field() ', 'def add_field(fieldname, func):\\n    \"\"\" \\n    This utility function checks for existence of a field in a dataset.\\n    If the field does not exist it is added by running func.\\n    \"\"\"\\n    if dataset.get_field(fieldname):\\n        logger.info(f\\'\"{fieldname}\" field already exists\\')\\n    else:\\n        logger.info(f\\'Adding \"{fieldname}\" field\\')\\n        func()\\n\\n# def add_new_field():\\n#     \"\"\" \\n#     Code for adding a field named \\'new\\' should be inserted in this function.\\n#     \"\"\"\\n#     pass\\n    \\n# add_field(\\'new\\', add_new_field)', 'def create_autocorrelated_images_view(threshold, delete=False):\\n    \"\"\" \\n    \"\"\"\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    sorted_by_timestamp_view = dataset.load_saved_view(\\'sorted_by_timestamp\\')\\n    view = sorted_by_timestamp_view.match(\\n        F(\\'similarity_with_prev_img\\') > threshold)\\n    dataset.save_view(\"autocorrelated_images_view\", view, overwrite=True)\\n    count = view.count()\\n    \\n    if delete:\\n        dataset.delete_samples(view) \\n        dataset.save()\\n     \\n    return count\\n  \\n# create_autocorrelated_images_view(0.98, True)', 'def count_ground_truth_bbs(dataset):\\n    total_detections = 0\\n    for sample in dataset:\\n        total_detections += len(sample.ground_truth.detections)\\n    return total_detections\\n\\n# count_ground_truth_bbs()', 'def create_bb_touching_edge_view(delete=False):\\n    \"\"\" \\n    https://docs.voxel51.com/recipes/remove_duplicate_annos.html\\n    \"\"\"\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    view = dataset.filter_labels(\\'ground_truth\\', \\n        (F(\\'bounding_box\\')[0] <= 0) | # left\\n        (F(\\'bounding_box\\')[1] <= 0) | # top\\n        ((F(\\'bounding_box\\')[0] + F(\\'bounding_box\\')[3]) >= 1) # right\\n    )\\n    dataset.save_view(\\'bb_touching_edge\\', view, overwrite=True) \\n    count = view.count()\\n           \\n    if delete:\\n        dataset.delete_labels(view)\\n    dataset.save()\\n            \\n    return  count\\n\\n# create_bb_touching_edge_view()', 'def remove_unannotated_images(yolo5_dataset_path: str) -> int:\\n    \"\"\" \\n    Removes unannoted images from a YOLO5 data set\\n    Arguments:\\n        yolo5_dataset_path -- absolute path for the YOLO5 dataset\\n    Returns:\\n        count -- number of image (*.jpg) and annotation file pairs removed\\n    \"\"\" \\n    search_str = f\\'{yolo5_dataset_path}/**/*.txt\\'\\n    txt_paths = glob.glob(search_str, recursive=True)\\n    count = 0\\n    for txt_path in txt_paths:\\n        if os.path.getsize(txt_path) == 0:\\n            img_path = txt_path.replace(\\'labels\\', \\'images\\').replace(\\'.txt\\', \\'.jpg\\')\\n            os.remove(txt_path)\\n            os.remove(img_path)\\n            count += 1\\n    return count\\n\\n# remove_unannotated_images(\\n#     yolo5_dataset_path=\\'/home/aubrey/myexport\\')', 'def export_51_to_YOLO(dataset_name: str, \\n                      export_dir: str, \\n                      remove_unannotated: bool) -> int:\\n    \"\"\"\\n    Export a dataset from 51 format to YOLO5 format.\\n    Optionally, unannotated images will be removed from the export_dir.\\n    \\n    Arguments:\\n    dataset_name -- a saved (persistent) 51 dataset\\n    export_dir -- absolute destination path for the YOLO5 dataset\\n    remove_unannoted -- if True, unannoted images are removed from the new YOLO5 dataset\\n\\n    Reference https://docs.voxel51.com/user_guide/export_datasets.html#yolov5dataset\\n    \"\"\"\\n    label_field = \"ground_truth\"\\n\\n    # The splits to export\\n    splits = [\"train\", \"val\"]\\n\\n    # All splits must use the same classes list\\n    classes = [\"live\", \"dead\", \"vcut\"]\\n\\n    # The dataset or view to export\\n    # We assume the dataset uses sample tags to encode the splits to export\\n    dataset_or_view = fo.load_dataset(dataset_name)\\n\\n    # Export the splits\\n    for split in splits:\\n        split_view = dataset_or_view.match_tags(split)\\n        split_view.export(\\n            export_dir=export_dir,\\n            dataset_type=fo.types.YOLOv5Dataset,\\n            label_field=label_field,\\n            split=split,\\n            classes=classes,\\n        )\\n        \\n    # Remove unannotated images (optional)\\n    images_removed = 0\\n    if remove_unannotated:\\n        images_removed = remove_unannotated_images(\\n            yolo5_dataset_path=export_dir)\\n    return images_removed     \\n\\n# export_51_to_YOLO(\\n#     dataset_name=\\'Guam07v3\\', \\n#     export_dir=\\'/home/aubrey/myexport\\', \\n#     remove_unannotated=True)', \"def train_model():\\n\\n    model = YOLO('/home/aubrey/label-studio-ml-backend/runs/detect/newt/weights/best.pt')\\n    results = model.train(\\n        resume = True,\\n        imgsz=1920,\\n        rect=True,\\n        # data= '/home/aubrey/myexport/dataset.yaml',\\n        epochs=5,\\n        batch=-1,\\n        patience=5,\\n        name='newt'\\n    )\\n\\n# train_model()\\n \\n# train model\\n# !yolo \\\\\\n# task=detect \\\\\\n# mode=train \\\\\\n# model= /home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks/runs/detect/train5/weights/best.pt \\\\\\n# imgsz=1920 \\\\\\n# data= /home/aubrey/myexport/dataset.yaml \\\\\\n# epochs=1000 \\\\\\n# batch=-1 \\\\\\n# patience=50 \\\\\\n# name=dataset3_yolov8n\", 'def launch_cvat(anno_key_suffix: str, view) -> str:\\n    \"\"\" \\n    Saves a FiftyOne view in CVAT and launches the CVAT annotator\\n    \\n    Arguments:\\n    anno_key_suffix - string     \\n    view - the view to be imported into CVAT\\n    \\n    Result:\\n    \\n    anno_key - a unique string in the form of myview-2024-11-27-16:57\\n    \"\"\"\\n    timestamp = datetime.strftime(datetime.now(), \\'%Y%m%d%H%M\\')\\n    anno_key = f\\'{anno_key_suffix}_{timestamp}\\'\\n    view.annotate(\\n        anno_key= anno_key,\\n        label_field=\"ground_truth\", \\n        launch_editor=True\\n    )\\n    return anno_key\\n    \\n# random_dozen_view = dataset.take(12)\\n# launch_cvat(\\'random_dozen\\', random_dozen_view)', 'def configure_logger(LOGFILE):\\n    \"\"\"\\n    Configure logger to send messages to notebook and LOGFILE\\n    \"\"\"\\n    logging.root.handlers = []\\n    logging.basicConfig(\\n        level=logging.INFO, \\n        format=\\'%(asctime)s %(message)s\\',\\n        datefmt=\\'%Y-%m-%d %H:%M:%S\\',\\n        handlers=[\\n            logging.FileHandler(filename=LOGFILE),\\n            logging.StreamHandler(sys.stdout)\\n        ]\\n    )\\n    logger = logging.getLogger()\\n    return logger', '# MAIN\\n\\n# Start of constants #############################################################################\\n\\n# path to dataset in new YOLO format \\nORIGINAL_DS_PATH = \\'/home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks\\'\\n\\n# path to latest weights file\\nORIGINAL_MODEL_PATH = f\\'{ORIGINAL_DS_PATH}/runs/detect/train5/weights/best.pt\\'\\n\\n# path to dataset in YOLOv5 format\\nNEW_DS_PATH = \\'/home/aubrey/datasets/Guam07v3\\'\\n\\n# name of FiftyOne dataset\\nFO_DATASET_NAME = \\'Guam07v3\\'\\n\\n# file name for log file saved in the same folder as this notebook\\nLOGFILE = \\'create_new_dataset.log\\'\\n\\n# Arguments for create_autocorrelated_images_view function.\\nAUTOCORRELATED_IMAGES_THRESHOLD = 0.98\\nDELETE_AUTOCORRELATED_IMAGES = True\\n\\n# Argument for create_autocorrelated_images_view function\\nDELETE_BBS_TOUCHING_EDGES = True\\n\\n# Option to retrain model. Usually FALSE.\\nRETRAIN_MODEL = False\\n\\n# Option to launch FiftyOne in browser at end of workflow. Usually True.\\nLAUNCH_51 = True\\n\\n# End of constants ########################################################################\\n\\n#configure logger\\nlogger = configure_logger(LOGFILE)\\n\\nlogger.info(globals())\\n\\n# update requirements.txt\\nlogger.info(\\'Updating \"requirements.txt\"\\')\\nupdate_requirements_file()\\n\\n# wrangle dataset into YOLOv5 format\\nif os.path.exists(NEW_DS_PATH):\\n    logger.info(f\\'\"{NEW_DS_PATH}\" already exists in YOLOv5 format\\')\\nelse:\\n    logger.info(f\\'creating dataset \"{NEW_DS_PATH}\" in YOLOv5 format\\')\\n    create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH)\\n\\n# Create new FiftyOne dataset\\nif FO_DATASET_NAME in fo.list_datasets():\\n    logger.info(f\\'FiftyOne dataset \"{FO_DATASET_NAME}\" already exists\\') \\nelse:\\n    logger.info(f\\'Creating FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\n    dataset = yolo2fiftyone(name=FO_DATASET_NAME, dataset_dir=NEW_DS_PATH)\\n    \\n# Load dataset\\nlogger.info(f\\'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\ndataset = fo.load_dataset(FO_DATASET_NAME)\\nlogger.info(f\\'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}\\')\\n\\n# Add fields if they don\\'t already exist\\nadd_field(\\'timestamp\\', add_timestamp_field)\\nadd_field(\\'embeddings\\', add_embeddings_field)\\nadd_field(\\'similarity_with_prev_img\\', add_similarity_with_prev_img_field)\\nadd_field(\\'yolov8\\', add_predictions_field)\\nadd_field(\\'mistakenness\\', add_mistakenness_field)\\n\\n# Find bounding boxes touching left, top or right edges of images\\nif \\'bb_touching_edge\\' in dataset.list_saved_views():\\n    logger.info(\\'\"bb_touching_edge_view\" already exists\\')\\nelse:\\n    logger.info(\\'Creating \"bb_touching_edge_view\"\\')\\n    if DELETE_BBS_TOUCHING_EDGES:\\n        logger.info(\\'    \"DELETE_BBS_TOUCHING_EDGES\" is True; bbs will be deleted\\')\\n    else:\\n        logger.info(\\'    \"DELETE_BBS_TOUCHING_EDGES\" is False; bbs will not be deleted\\')\\n    bb_touching_edge_count = create_bb_touching_edge_view(DELETE_BBS_TOUCHING_EDGES)\\n    logger.info(f\\'    {bb_touching_edge_count} ground truth bounding boxes touching image edges were found\\')\\n\\n# Find autocorrelated images\\nif \\'autocorrelated_images_view\\' in dataset.list_saved_views():\\n    logger.info(\\'\"autocorrelated_images_view\" already exists\\')\\nelse:\\n    logger.info(\\'Creating \"autocorrelated_images_view\"\\')\\n    if DELETE_BBS_TOUCHING_EDGES:\\n        logger.info(\\'    \"DELETE_AUTOCORRELATED_IMAGES\" is True; samples will be deleted\\')\\n    else:\\n        logger.info(\\'    \"DELETE_AUTOCORRELATED_IMAGES\" is False; bbs will not be deleted\\')\\n    autocorrelated_image_count = create_autocorrelated_images_view(\\n        threshold=AUTOCORRELATED_IMAGES_THRESHOLD, delete=DELETE_AUTOCORRELATED_IMAGES)\\n    logger.info(f\\'    With a threshold of {AUTOCORRELATED_IMAGES_THRESHOLD}, {autocorrelated_image_count} autocorrelated images were found\\')\\n\\nif RETRAIN_MODEL:\\n    export_51_to_YOLO(\\n        dataset_name=\\'Guam07v3\\', \\n        export_dir=\\'/home/aubrey/myexport\\', \\n        remove_unannotated=True)\\n    train_model()\\n\\nif LAUNCH_51:\\n    \\n    # Reload dataset\\n    logger.info(f\\'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    logger.info(f\\'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}\\')\\n\\n    # Launch FiftyOne app in browser\\n    logger.info(f\\'Launching FifyOne app in browser\\')\\n    session = fo.launch_app(dataset, auto=False)\\n    logger.info(session)\\n\\nlogger.info(\\'FINISHED\\')'], '_oh': {}, '_dh': [PosixPath('/home/aubrey/datasets/dataset5/CRB-Damage-Dataset-Improvement/code')], 'In': ['', 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob\\nimport mysecrets\\n# import ipywidgets as widgets\\n# from IPython.display import display', 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob', 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob', 'def add_timestamp_field():\\n    dataset.add_sample_field(\"timestamp\", fo.DateTimeField)\\n\\n    for sample in dataset:\\n        timestamp_str = os.path.basename(sample.filepath)[4:-4]\\n        dt = datetime.strptime(timestamp_str, \\'%Y%m%d_%H%M%S\\')\\n        # ic(timestamp_str, dt)\\n        sample[\\'timestamp\\'] = dt\\n        sample.save()\\n    \\n    # Create view  \\n    view = dataset.sort_by(F\\'timestamp\\')\\n    dataset.save_view(\\'sorted_by_timestamp\\', view, overwrite=True)', \"def update_requirements_file():\\n    os.system('pip list --format=freeze > requirements.txt')\\n\\n# update_requirements_file()\", 'def create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH):\\n    \"\"\" \\n    \"\"\"\\n    os.mkdir(NEW_DS_PATH)\\n    os.mkdir(f\\'{NEW_DS_PATH}/images\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/images/train\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/images/val\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels/train\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels/val\\')\\n    \\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/train/*.jpg\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/images/train\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/train/*.txt\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/labels/train\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/val/*.jpg\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/images/val\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/val/*.txt\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/labels/val\\')\\n        \\n    s = f\\'path: {NEW_DS_PATH} \\\\n\\'\\n    s += \\'train: ./images/train/ \\\\n\\'\\n    s += \\'val: ./images/val/ \\\\n\\'\\n    s += \\'names: \\\\n\\'\\n    s += \\'  0: live \\\\n\\'\\n    s += \\'  1: dead \\\\n\\'\\n    s += \\'  2: vcut \\\\n\\'\\n    with open(f\\'{NEW_DS_PATH}/dataset.yaml\\', \\'w\\') as f:\\n        f.write(s)', 'def yolo2fiftyone(FO_DATASET_NAME, dataset_dir, splits=[\"train\", \"val\"]):\\n    \"\"\" \\n    Imports a dataset in YOLO5 format into FiftyOne, using tags to mark the samples in each split \\n    \"\"\" \\n    dataset = fo.Dataset(name, persistent=True)\\n    for split in splits:\\n        dataset.add_dir(\\n            dataset_dir=dataset_dir,\\n            dataset_type=fo.types.YOLOv5Dataset,\\n            split=split,\\n            tags=split,\\n    )\\n    return dataset', 'def add_embeddings_field():\\n    \"\"\" \\n    \"\"\" \\n    model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\\n    dataset.compute_embeddings(model=model, embeddings_field=\\'embeddings\\')', 'def cosine_similarity(a, b):\\n    return np.dot(a,b)/(norm(a)*norm(b))\\n \\n# a = np.array([2,1,2,3,2,9])\\n# b = np.array([3,4,2,4,5,5])\\n# cosine_similarity(a, b)', 'def add_similarity_with_prev_img_field():\\n    \"\"\" \\n    \"\"\"\\n    view = dataset.load_saved_view(\\'sorted_by_timestamp\\')\\n    # thresh = 0.92\\n    first_sample = True\\n    for sample in view:\\n        if first_sample:\\n            current_embeddings = sample.embeddings\\n            similarity = 0.0\\n            first_sample = False\\n        else:\\n            previous_embeddings = current_embeddings\\n            current_embeddings = sample.embeddings\\n            similarity = cosine_similarity(previous_embeddings, current_embeddings)\\n        sample[\\'similarity_with_prev_img\\'] = similarity\\n        # if similarity > thresh:\\n        #     sample.tags.append(f\\'similarity>{thresh}\\')\\n        # else:\\n        #     sample.tags.append(\\'similarity OK\\') \\n        sample.save()', 'def add_predictions_field():\\n    \"\"\" \\n    \"\"\"\\n    # Load YOLOv8 model\\n    # from ultralytics import YOLO\\n    model = YOLO(ORIGINAL_MODEL_PATH)\\n    dataset.apply_model(model, label_field=\"yolov8\")\\n    \\n# add_predictions_field()', 'def add_mistakenness_field():\\n    \"\"\" \\n    Adds mistakenness, possible_missing and possible_spurious fields.\\n    See docs at https://docs.voxel51.com/brain.html#label-mistakes for details.\\n    \"\"\"\\n    fob.compute_mistakenness(dataset, \"yolov8\", label_field=\"ground_truth\")  \\n    \\n# add_mistakenness_field() ', 'def add_field(fieldname, func):\\n    \"\"\" \\n    This utility function checks for existence of a field in a dataset.\\n    If the field does not exist it is added by running func.\\n    \"\"\"\\n    if dataset.get_field(fieldname):\\n        logger.info(f\\'\"{fieldname}\" field already exists\\')\\n    else:\\n        logger.info(f\\'Adding \"{fieldname}\" field\\')\\n        func()\\n\\n# def add_new_field():\\n#     \"\"\" \\n#     Code for adding a field named \\'new\\' should be inserted in this function.\\n#     \"\"\"\\n#     pass\\n    \\n# add_field(\\'new\\', add_new_field)', 'def create_autocorrelated_images_view(threshold, delete=False):\\n    \"\"\" \\n    \"\"\"\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    sorted_by_timestamp_view = dataset.load_saved_view(\\'sorted_by_timestamp\\')\\n    view = sorted_by_timestamp_view.match(\\n        F(\\'similarity_with_prev_img\\') > threshold)\\n    dataset.save_view(\"autocorrelated_images_view\", view, overwrite=True)\\n    count = view.count()\\n    \\n    if delete:\\n        dataset.delete_samples(view) \\n        dataset.save()\\n     \\n    return count\\n  \\n# create_autocorrelated_images_view(0.98, True)', 'def count_ground_truth_bbs(dataset):\\n    total_detections = 0\\n    for sample in dataset:\\n        total_detections += len(sample.ground_truth.detections)\\n    return total_detections\\n\\n# count_ground_truth_bbs()', 'def create_bb_touching_edge_view(delete=False):\\n    \"\"\" \\n    https://docs.voxel51.com/recipes/remove_duplicate_annos.html\\n    \"\"\"\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    view = dataset.filter_labels(\\'ground_truth\\', \\n        (F(\\'bounding_box\\')[0] <= 0) | # left\\n        (F(\\'bounding_box\\')[1] <= 0) | # top\\n        ((F(\\'bounding_box\\')[0] + F(\\'bounding_box\\')[3]) >= 1) # right\\n    )\\n    dataset.save_view(\\'bb_touching_edge\\', view, overwrite=True) \\n    count = view.count()\\n           \\n    if delete:\\n        dataset.delete_labels(view)\\n    dataset.save()\\n            \\n    return  count\\n\\n# create_bb_touching_edge_view()', 'def remove_unannotated_images(yolo5_dataset_path: str) -> int:\\n    \"\"\" \\n    Removes unannoted images from a YOLO5 data set\\n    Arguments:\\n        yolo5_dataset_path -- absolute path for the YOLO5 dataset\\n    Returns:\\n        count -- number of image (*.jpg) and annotation file pairs removed\\n    \"\"\" \\n    search_str = f\\'{yolo5_dataset_path}/**/*.txt\\'\\n    txt_paths = glob.glob(search_str, recursive=True)\\n    count = 0\\n    for txt_path in txt_paths:\\n        if os.path.getsize(txt_path) == 0:\\n            img_path = txt_path.replace(\\'labels\\', \\'images\\').replace(\\'.txt\\', \\'.jpg\\')\\n            os.remove(txt_path)\\n            os.remove(img_path)\\n            count += 1\\n    return count\\n\\n# remove_unannotated_images(\\n#     yolo5_dataset_path=\\'/home/aubrey/myexport\\')', 'def export_51_to_YOLO(dataset_name: str, \\n                      export_dir: str, \\n                      remove_unannotated: bool) -> int:\\n    \"\"\"\\n    Export a dataset from 51 format to YOLO5 format.\\n    Optionally, unannotated images will be removed from the export_dir.\\n    \\n    Arguments:\\n    dataset_name -- a saved (persistent) 51 dataset\\n    export_dir -- absolute destination path for the YOLO5 dataset\\n    remove_unannoted -- if True, unannoted images are removed from the new YOLO5 dataset\\n\\n    Reference https://docs.voxel51.com/user_guide/export_datasets.html#yolov5dataset\\n    \"\"\"\\n    label_field = \"ground_truth\"\\n\\n    # The splits to export\\n    splits = [\"train\", \"val\"]\\n\\n    # All splits must use the same classes list\\n    classes = [\"live\", \"dead\", \"vcut\"]\\n\\n    # The dataset or view to export\\n    # We assume the dataset uses sample tags to encode the splits to export\\n    dataset_or_view = fo.load_dataset(dataset_name)\\n\\n    # Export the splits\\n    for split in splits:\\n        split_view = dataset_or_view.match_tags(split)\\n        split_view.export(\\n            export_dir=export_dir,\\n            dataset_type=fo.types.YOLOv5Dataset,\\n            label_field=label_field,\\n            split=split,\\n            classes=classes,\\n        )\\n        \\n    # Remove unannotated images (optional)\\n    images_removed = 0\\n    if remove_unannotated:\\n        images_removed = remove_unannotated_images(\\n            yolo5_dataset_path=export_dir)\\n    return images_removed     \\n\\n# export_51_to_YOLO(\\n#     dataset_name=\\'Guam07v3\\', \\n#     export_dir=\\'/home/aubrey/myexport\\', \\n#     remove_unannotated=True)', \"def train_model():\\n\\n    model = YOLO('/home/aubrey/label-studio-ml-backend/runs/detect/newt/weights/best.pt')\\n    results = model.train(\\n        resume = True,\\n        imgsz=1920,\\n        rect=True,\\n        # data= '/home/aubrey/myexport/dataset.yaml',\\n        epochs=5,\\n        batch=-1,\\n        patience=5,\\n        name='newt'\\n    )\\n\\n# train_model()\\n \\n# train model\\n# !yolo \\\\\\n# task=detect \\\\\\n# mode=train \\\\\\n# model= /home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks/runs/detect/train5/weights/best.pt \\\\\\n# imgsz=1920 \\\\\\n# data= /home/aubrey/myexport/dataset.yaml \\\\\\n# epochs=1000 \\\\\\n# batch=-1 \\\\\\n# patience=50 \\\\\\n# name=dataset3_yolov8n\", 'def launch_cvat(anno_key_suffix: str, view) -> str:\\n    \"\"\" \\n    Saves a FiftyOne view in CVAT and launches the CVAT annotator\\n    \\n    Arguments:\\n    anno_key_suffix - string     \\n    view - the view to be imported into CVAT\\n    \\n    Result:\\n    \\n    anno_key - a unique string in the form of myview-2024-11-27-16:57\\n    \"\"\"\\n    timestamp = datetime.strftime(datetime.now(), \\'%Y%m%d%H%M\\')\\n    anno_key = f\\'{anno_key_suffix}_{timestamp}\\'\\n    view.annotate(\\n        anno_key= anno_key,\\n        label_field=\"ground_truth\", \\n        launch_editor=True\\n    )\\n    return anno_key\\n    \\n# random_dozen_view = dataset.take(12)\\n# launch_cvat(\\'random_dozen\\', random_dozen_view)', 'def configure_logger(LOGFILE):\\n    \"\"\"\\n    Configure logger to send messages to notebook and LOGFILE\\n    \"\"\"\\n    logging.root.handlers = []\\n    logging.basicConfig(\\n        level=logging.INFO, \\n        format=\\'%(asctime)s %(message)s\\',\\n        datefmt=\\'%Y-%m-%d %H:%M:%S\\',\\n        handlers=[\\n            logging.FileHandler(filename=LOGFILE),\\n            logging.StreamHandler(sys.stdout)\\n        ]\\n    )\\n    logger = logging.getLogger()\\n    return logger', '# MAIN\\n\\n# Start of constants #############################################################################\\n\\n# path to dataset in new YOLO format \\nORIGINAL_DS_PATH = \\'/home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks\\'\\n\\n# path to latest weights file\\nORIGINAL_MODEL_PATH = f\\'{ORIGINAL_DS_PATH}/runs/detect/train5/weights/best.pt\\'\\n\\n# path to dataset in YOLOv5 format\\nNEW_DS_PATH = \\'/home/aubrey/datasets/Guam07v3\\'\\n\\n# name of FiftyOne dataset\\nFO_DATASET_NAME = \\'Guam07v3\\'\\n\\n# file name for log file saved in the same folder as this notebook\\nLOGFILE = \\'create_new_dataset.log\\'\\n\\n# Arguments for create_autocorrelated_images_view function.\\nAUTOCORRELATED_IMAGES_THRESHOLD = 0.98\\nDELETE_AUTOCORRELATED_IMAGES = True\\n\\n# Argument for create_autocorrelated_images_view function\\nDELETE_BBS_TOUCHING_EDGES = True\\n\\n# Option to retrain model. Usually FALSE.\\nRETRAIN_MODEL = False\\n\\n# Option to launch FiftyOne in browser at end of workflow. Usually True.\\nLAUNCH_51 = True\\n\\n# End of constants ########################################################################\\n\\n#configure logger\\nlogger = configure_logger(LOGFILE)\\n\\nlogger.info(globals())\\n\\n# update requirements.txt\\nlogger.info(\\'Updating \"requirements.txt\"\\')\\nupdate_requirements_file()\\n\\n# wrangle dataset into YOLOv5 format\\nif os.path.exists(NEW_DS_PATH):\\n    logger.info(f\\'\"{NEW_DS_PATH}\" already exists in YOLOv5 format\\')\\nelse:\\n    logger.info(f\\'creating dataset \"{NEW_DS_PATH}\" in YOLOv5 format\\')\\n    create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH)\\n\\n# Create new FiftyOne dataset\\nif FO_DATASET_NAME in fo.list_datasets():\\n    logger.info(f\\'FiftyOne dataset \"{FO_DATASET_NAME}\" already exists\\') \\nelse:\\n    logger.info(f\\'Creating FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\n    dataset = yolo2fiftyone(name=FO_DATASET_NAME, dataset_dir=NEW_DS_PATH)\\n    \\n# Load dataset\\nlogger.info(f\\'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\ndataset = fo.load_dataset(FO_DATASET_NAME)\\nlogger.info(f\\'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}\\')\\n\\n# Add fields if they don\\'t already exist\\nadd_field(\\'timestamp\\', add_timestamp_field)\\nadd_field(\\'embeddings\\', add_embeddings_field)\\nadd_field(\\'similarity_with_prev_img\\', add_similarity_with_prev_img_field)\\nadd_field(\\'yolov8\\', add_predictions_field)\\nadd_field(\\'mistakenness\\', add_mistakenness_field)\\n\\n# Find bounding boxes touching left, top or right edges of images\\nif \\'bb_touching_edge\\' in dataset.list_saved_views():\\n    logger.info(\\'\"bb_touching_edge_view\" already exists\\')\\nelse:\\n    logger.info(\\'Creating \"bb_touching_edge_view\"\\')\\n    if DELETE_BBS_TOUCHING_EDGES:\\n        logger.info(\\'    \"DELETE_BBS_TOUCHING_EDGES\" is True; bbs will be deleted\\')\\n    else:\\n        logger.info(\\'    \"DELETE_BBS_TOUCHING_EDGES\" is False; bbs will not be deleted\\')\\n    bb_touching_edge_count = create_bb_touching_edge_view(DELETE_BBS_TOUCHING_EDGES)\\n    logger.info(f\\'    {bb_touching_edge_count} ground truth bounding boxes touching image edges were found\\')\\n\\n# Find autocorrelated images\\nif \\'autocorrelated_images_view\\' in dataset.list_saved_views():\\n    logger.info(\\'\"autocorrelated_images_view\" already exists\\')\\nelse:\\n    logger.info(\\'Creating \"autocorrelated_images_view\"\\')\\n    if DELETE_BBS_TOUCHING_EDGES:\\n        logger.info(\\'    \"DELETE_AUTOCORRELATED_IMAGES\" is True; samples will be deleted\\')\\n    else:\\n        logger.info(\\'    \"DELETE_AUTOCORRELATED_IMAGES\" is False; bbs will not be deleted\\')\\n    autocorrelated_image_count = create_autocorrelated_images_view(\\n        threshold=AUTOCORRELATED_IMAGES_THRESHOLD, delete=DELETE_AUTOCORRELATED_IMAGES)\\n    logger.info(f\\'    With a threshold of {AUTOCORRELATED_IMAGES_THRESHOLD}, {autocorrelated_image_count} autocorrelated images were found\\')\\n\\nif RETRAIN_MODEL:\\n    export_51_to_YOLO(\\n        dataset_name=\\'Guam07v3\\', \\n        export_dir=\\'/home/aubrey/myexport\\', \\n        remove_unannotated=True)\\n    train_model()\\n\\nif LAUNCH_51:\\n    \\n    # Reload dataset\\n    logger.info(f\\'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    logger.info(f\\'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}\\')\\n\\n    # Launch FiftyOne app in browser\\n    logger.info(f\\'Launching FifyOne app in browser\\')\\n    session = fo.launch_app(dataset, auto=False)\\n    logger.info(session)\\n\\nlogger.info(\\'FINISHED\\')'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x74cf82960760>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x74cf829613f0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x74cf829613f0>, 'open': <function open at 0x74cf8529f0a0>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/home/aubrey/datasets/dataset5/CRB-Damage-Dataset-Improvement/code/create_new_dataset.ipynb', '_i': 'def configure_logger(LOGFILE):\\n    \"\"\"\\n    Configure logger to send messages to notebook and LOGFILE\\n    \"\"\"\\n    logging.root.handlers = []\\n    logging.basicConfig(\\n        level=logging.INFO, \\n        format=\\'%(asctime)s %(message)s\\',\\n        datefmt=\\'%Y-%m-%d %H:%M:%S\\',\\n        handlers=[\\n            logging.FileHandler(filename=LOGFILE),\\n            logging.StreamHandler(sys.stdout)\\n        ]\\n    )\\n    logger = logging.getLogger()\\n    return logger', '_ii': 'def launch_cvat(anno_key_suffix: str, view) -> str:\\n    \"\"\" \\n    Saves a FiftyOne view in CVAT and launches the CVAT annotator\\n    \\n    Arguments:\\n    anno_key_suffix - string     \\n    view - the view to be imported into CVAT\\n    \\n    Result:\\n    \\n    anno_key - a unique string in the form of myview-2024-11-27-16:57\\n    \"\"\"\\n    timestamp = datetime.strftime(datetime.now(), \\'%Y%m%d%H%M\\')\\n    anno_key = f\\'{anno_key_suffix}_{timestamp}\\'\\n    view.annotate(\\n        anno_key= anno_key,\\n        label_field=\"ground_truth\", \\n        launch_editor=True\\n    )\\n    return anno_key\\n    \\n# random_dozen_view = dataset.take(12)\\n# launch_cvat(\\'random_dozen\\', random_dozen_view)', '_iii': \"def train_model():\\n\\n    model = YOLO('/home/aubrey/label-studio-ml-backend/runs/detect/newt/weights/best.pt')\\n    results = model.train(\\n        resume = True,\\n        imgsz=1920,\\n        rect=True,\\n        # data= '/home/aubrey/myexport/dataset.yaml',\\n        epochs=5,\\n        batch=-1,\\n        patience=5,\\n        name='newt'\\n    )\\n\\n# train_model()\\n \\n# train model\\n# !yolo \\\\\\n# task=detect \\\\\\n# mode=train \\\\\\n# model= /home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks/runs/detect/train5/weights/best.pt \\\\\\n# imgsz=1920 \\\\\\n# data= /home/aubrey/myexport/dataset.yaml \\\\\\n# epochs=1000 \\\\\\n# batch=-1 \\\\\\n# patience=50 \\\\\\n# name=dataset3_yolov8n\", '_i1': 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob\\nimport mysecrets\\n# import ipywidgets as widgets\\n# from IPython.display import display', 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'shutil': <module 'shutil' from '/usr/lib/python3.10/shutil.py'>, 'glob': <module 'glob' from '/usr/lib/python3.10/glob.py'>, 'fo': <module 'fiftyone' from '/home/aubrey/datasets/dataset5/CRB-Damage-Dataset-Improvement/.venv/lib/python3.10/site-packages/fiftyone/__init__.py'>, 'fob': <module 'fiftyone.brain' from '/home/aubrey/datasets/dataset5/CRB-Damage-Dataset-Improvement/.venv/lib/python3.10/site-packages/fiftyone/brain/__init__.py'>, 'foz': <module 'fiftyone.zoo' from '/home/aubrey/datasets/dataset5/CRB-Damage-Dataset-Improvement/.venv/lib/python3.10/site-packages/fiftyone/zoo/__init__.py'>, 'F': <class 'fiftyone.core.expressions.ViewField'>, 'logging': <module 'logging' from '/usr/lib/python3.10/logging/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'ic': <icecream.icecream.IceCreamDebugger object at 0x74cef47120b0>, 'datetime': <class 'datetime.datetime'>, 'np': <module 'numpy' from '/home/aubrey/datasets/dataset5/CRB-Damage-Dataset-Improvement/.venv/lib/python3.10/site-packages/numpy/__init__.py'>, 'norm': <function norm at 0x74cf7bf20df0>, 'YOLO': <class 'ultralytics.models.yolo.model.YOLO'>, '_i2': 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob', '_i3': 'import os\\nimport shutil\\nimport glob\\nimport fiftyone as fo\\nimport fiftyone.brain as fob\\nimport fiftyone.zoo as foz\\nfrom fiftyone import ViewField as F\\nimport logging\\nimport sys\\nfrom icecream import ic\\nfrom datetime import datetime\\nimport numpy as np\\nfrom numpy.linalg import norm\\nfrom ultralytics import YOLO\\nimport glob', '_i4': 'def add_timestamp_field():\\n    dataset.add_sample_field(\"timestamp\", fo.DateTimeField)\\n\\n    for sample in dataset:\\n        timestamp_str = os.path.basename(sample.filepath)[4:-4]\\n        dt = datetime.strptime(timestamp_str, \\'%Y%m%d_%H%M%S\\')\\n        # ic(timestamp_str, dt)\\n        sample[\\'timestamp\\'] = dt\\n        sample.save()\\n    \\n    # Create view  \\n    view = dataset.sort_by(F\\'timestamp\\')\\n    dataset.save_view(\\'sorted_by_timestamp\\', view, overwrite=True)', 'add_timestamp_field': <function add_timestamp_field at 0x74cf82991a20>, '_i5': \"def update_requirements_file():\\n    os.system('pip list --format=freeze > requirements.txt')\\n\\n# update_requirements_file()\", 'update_requirements_file': <function update_requirements_file at 0x74cf82991990>, '_i6': 'def create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH):\\n    \"\"\" \\n    \"\"\"\\n    os.mkdir(NEW_DS_PATH)\\n    os.mkdir(f\\'{NEW_DS_PATH}/images\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/images/train\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/images/val\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels/train\\')\\n    os.mkdir(f\\'{NEW_DS_PATH}/labels/val\\')\\n    \\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/train/*.jpg\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/images/train\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/train/*.txt\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/labels/train\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/val/*.jpg\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/images/val\\')\\n    for filepath in glob.glob(f\\'{ORIGINAL_DS_PATH}/val/*.txt\\'):\\n        shutil.copy2(filepath, f\\'{NEW_DS_PATH}/labels/val\\')\\n        \\n    s = f\\'path: {NEW_DS_PATH} \\\\n\\'\\n    s += \\'train: ./images/train/ \\\\n\\'\\n    s += \\'val: ./images/val/ \\\\n\\'\\n    s += \\'names: \\\\n\\'\\n    s += \\'  0: live \\\\n\\'\\n    s += \\'  1: dead \\\\n\\'\\n    s += \\'  2: vcut \\\\n\\'\\n    with open(f\\'{NEW_DS_PATH}/dataset.yaml\\', \\'w\\') as f:\\n        f.write(s)', 'create_new_dataset': <function create_new_dataset at 0x74ce36169990>, '_i7': 'def yolo2fiftyone(FO_DATASET_NAME, dataset_dir, splits=[\"train\", \"val\"]):\\n    \"\"\" \\n    Imports a dataset in YOLO5 format into FiftyOne, using tags to mark the samples in each split \\n    \"\"\" \\n    dataset = fo.Dataset(name, persistent=True)\\n    for split in splits:\\n        dataset.add_dir(\\n            dataset_dir=dataset_dir,\\n            dataset_type=fo.types.YOLOv5Dataset,\\n            split=split,\\n            tags=split,\\n    )\\n    return dataset', 'yolo2fiftyone': <function yolo2fiftyone at 0x74ce361696c0>, '_i8': 'def add_embeddings_field():\\n    \"\"\" \\n    \"\"\" \\n    model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\\n    dataset.compute_embeddings(model=model, embeddings_field=\\'embeddings\\')', 'add_embeddings_field': <function add_embeddings_field at 0x74ce36169a20>, '_i9': 'def cosine_similarity(a, b):\\n    return np.dot(a,b)/(norm(a)*norm(b))\\n \\n# a = np.array([2,1,2,3,2,9])\\n# b = np.array([3,4,2,4,5,5])\\n# cosine_similarity(a, b)', 'cosine_similarity': <function cosine_similarity at 0x74ce36169510>, '_i10': 'def add_similarity_with_prev_img_field():\\n    \"\"\" \\n    \"\"\"\\n    view = dataset.load_saved_view(\\'sorted_by_timestamp\\')\\n    # thresh = 0.92\\n    first_sample = True\\n    for sample in view:\\n        if first_sample:\\n            current_embeddings = sample.embeddings\\n            similarity = 0.0\\n            first_sample = False\\n        else:\\n            previous_embeddings = current_embeddings\\n            current_embeddings = sample.embeddings\\n            similarity = cosine_similarity(previous_embeddings, current_embeddings)\\n        sample[\\'similarity_with_prev_img\\'] = similarity\\n        # if similarity > thresh:\\n        #     sample.tags.append(f\\'similarity>{thresh}\\')\\n        # else:\\n        #     sample.tags.append(\\'similarity OK\\') \\n        sample.save()', 'add_similarity_with_prev_img_field': <function add_similarity_with_prev_img_field at 0x74ce36169cf0>, '_i11': 'def add_predictions_field():\\n    \"\"\" \\n    \"\"\"\\n    # Load YOLOv8 model\\n    # from ultralytics import YOLO\\n    model = YOLO(ORIGINAL_MODEL_PATH)\\n    dataset.apply_model(model, label_field=\"yolov8\")\\n    \\n# add_predictions_field()', 'add_predictions_field': <function add_predictions_field at 0x74ce36168b80>, '_i12': 'def add_mistakenness_field():\\n    \"\"\" \\n    Adds mistakenness, possible_missing and possible_spurious fields.\\n    See docs at https://docs.voxel51.com/brain.html#label-mistakes for details.\\n    \"\"\"\\n    fob.compute_mistakenness(dataset, \"yolov8\", label_field=\"ground_truth\")  \\n    \\n# add_mistakenness_field() ', 'add_mistakenness_field': <function add_mistakenness_field at 0x74ce36169c60>, '_i13': 'def add_field(fieldname, func):\\n    \"\"\" \\n    This utility function checks for existence of a field in a dataset.\\n    If the field does not exist it is added by running func.\\n    \"\"\"\\n    if dataset.get_field(fieldname):\\n        logger.info(f\\'\"{fieldname}\" field already exists\\')\\n    else:\\n        logger.info(f\\'Adding \"{fieldname}\" field\\')\\n        func()\\n\\n# def add_new_field():\\n#     \"\"\" \\n#     Code for adding a field named \\'new\\' should be inserted in this function.\\n#     \"\"\"\\n#     pass\\n    \\n# add_field(\\'new\\', add_new_field)', 'add_field': <function add_field at 0x74ce36169f30>, '_i14': 'def create_autocorrelated_images_view(threshold, delete=False):\\n    \"\"\" \\n    \"\"\"\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    sorted_by_timestamp_view = dataset.load_saved_view(\\'sorted_by_timestamp\\')\\n    view = sorted_by_timestamp_view.match(\\n        F(\\'similarity_with_prev_img\\') > threshold)\\n    dataset.save_view(\"autocorrelated_images_view\", view, overwrite=True)\\n    count = view.count()\\n    \\n    if delete:\\n        dataset.delete_samples(view) \\n        dataset.save()\\n     \\n    return count\\n  \\n# create_autocorrelated_images_view(0.98, True)', 'create_autocorrelated_images_view': <function create_autocorrelated_images_view at 0x74ce36169ea0>, '_i15': 'def count_ground_truth_bbs(dataset):\\n    total_detections = 0\\n    for sample in dataset:\\n        total_detections += len(sample.ground_truth.detections)\\n    return total_detections\\n\\n# count_ground_truth_bbs()', 'count_ground_truth_bbs': <function count_ground_truth_bbs at 0x74ce361693f0>, '_i16': 'def create_bb_touching_edge_view(delete=False):\\n    \"\"\" \\n    https://docs.voxel51.com/recipes/remove_duplicate_annos.html\\n    \"\"\"\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    view = dataset.filter_labels(\\'ground_truth\\', \\n        (F(\\'bounding_box\\')[0] <= 0) | # left\\n        (F(\\'bounding_box\\')[1] <= 0) | # top\\n        ((F(\\'bounding_box\\')[0] + F(\\'bounding_box\\')[3]) >= 1) # right\\n    )\\n    dataset.save_view(\\'bb_touching_edge\\', view, overwrite=True) \\n    count = view.count()\\n           \\n    if delete:\\n        dataset.delete_labels(view)\\n    dataset.save()\\n            \\n    return  count\\n\\n# create_bb_touching_edge_view()', 'create_bb_touching_edge_view': <function create_bb_touching_edge_view at 0x74ce36169120>, '_i17': 'def remove_unannotated_images(yolo5_dataset_path: str) -> int:\\n    \"\"\" \\n    Removes unannoted images from a YOLO5 data set\\n    Arguments:\\n        yolo5_dataset_path -- absolute path for the YOLO5 dataset\\n    Returns:\\n        count -- number of image (*.jpg) and annotation file pairs removed\\n    \"\"\" \\n    search_str = f\\'{yolo5_dataset_path}/**/*.txt\\'\\n    txt_paths = glob.glob(search_str, recursive=True)\\n    count = 0\\n    for txt_path in txt_paths:\\n        if os.path.getsize(txt_path) == 0:\\n            img_path = txt_path.replace(\\'labels\\', \\'images\\').replace(\\'.txt\\', \\'.jpg\\')\\n            os.remove(txt_path)\\n            os.remove(img_path)\\n            count += 1\\n    return count\\n\\n# remove_unannotated_images(\\n#     yolo5_dataset_path=\\'/home/aubrey/myexport\\')', 'remove_unannotated_images': <function remove_unannotated_images at 0x74ce36169fc0>, '_i18': 'def export_51_to_YOLO(dataset_name: str, \\n                      export_dir: str, \\n                      remove_unannotated: bool) -> int:\\n    \"\"\"\\n    Export a dataset from 51 format to YOLO5 format.\\n    Optionally, unannotated images will be removed from the export_dir.\\n    \\n    Arguments:\\n    dataset_name -- a saved (persistent) 51 dataset\\n    export_dir -- absolute destination path for the YOLO5 dataset\\n    remove_unannoted -- if True, unannoted images are removed from the new YOLO5 dataset\\n\\n    Reference https://docs.voxel51.com/user_guide/export_datasets.html#yolov5dataset\\n    \"\"\"\\n    label_field = \"ground_truth\"\\n\\n    # The splits to export\\n    splits = [\"train\", \"val\"]\\n\\n    # All splits must use the same classes list\\n    classes = [\"live\", \"dead\", \"vcut\"]\\n\\n    # The dataset or view to export\\n    # We assume the dataset uses sample tags to encode the splits to export\\n    dataset_or_view = fo.load_dataset(dataset_name)\\n\\n    # Export the splits\\n    for split in splits:\\n        split_view = dataset_or_view.match_tags(split)\\n        split_view.export(\\n            export_dir=export_dir,\\n            dataset_type=fo.types.YOLOv5Dataset,\\n            label_field=label_field,\\n            split=split,\\n            classes=classes,\\n        )\\n        \\n    # Remove unannotated images (optional)\\n    images_removed = 0\\n    if remove_unannotated:\\n        images_removed = remove_unannotated_images(\\n            yolo5_dataset_path=export_dir)\\n    return images_removed     \\n\\n# export_51_to_YOLO(\\n#     dataset_name=\\'Guam07v3\\', \\n#     export_dir=\\'/home/aubrey/myexport\\', \\n#     remove_unannotated=True)', 'export_51_to_YOLO': <function export_51_to_YOLO at 0x74ce3616a050>, '_i19': \"def train_model():\\n\\n    model = YOLO('/home/aubrey/label-studio-ml-backend/runs/detect/newt/weights/best.pt')\\n    results = model.train(\\n        resume = True,\\n        imgsz=1920,\\n        rect=True,\\n        # data= '/home/aubrey/myexport/dataset.yaml',\\n        epochs=5,\\n        batch=-1,\\n        patience=5,\\n        name='newt'\\n    )\\n\\n# train_model()\\n \\n# train model\\n# !yolo \\\\\\n# task=detect \\\\\\n# mode=train \\\\\\n# model= /home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks/runs/detect/train5/weights/best.pt \\\\\\n# imgsz=1920 \\\\\\n# data= /home/aubrey/myexport/dataset.yaml \\\\\\n# epochs=1000 \\\\\\n# batch=-1 \\\\\\n# patience=50 \\\\\\n# name=dataset3_yolov8n\", 'train_model': <function train_model at 0x74ce3616a0e0>, '_i20': 'def launch_cvat(anno_key_suffix: str, view) -> str:\\n    \"\"\" \\n    Saves a FiftyOne view in CVAT and launches the CVAT annotator\\n    \\n    Arguments:\\n    anno_key_suffix - string     \\n    view - the view to be imported into CVAT\\n    \\n    Result:\\n    \\n    anno_key - a unique string in the form of myview-2024-11-27-16:57\\n    \"\"\"\\n    timestamp = datetime.strftime(datetime.now(), \\'%Y%m%d%H%M\\')\\n    anno_key = f\\'{anno_key_suffix}_{timestamp}\\'\\n    view.annotate(\\n        anno_key= anno_key,\\n        label_field=\"ground_truth\", \\n        launch_editor=True\\n    )\\n    return anno_key\\n    \\n# random_dozen_view = dataset.take(12)\\n# launch_cvat(\\'random_dozen\\', random_dozen_view)', 'launch_cvat': <function launch_cvat at 0x74ce3616a560>, '_i21': 'def configure_logger(LOGFILE):\\n    \"\"\"\\n    Configure logger to send messages to notebook and LOGFILE\\n    \"\"\"\\n    logging.root.handlers = []\\n    logging.basicConfig(\\n        level=logging.INFO, \\n        format=\\'%(asctime)s %(message)s\\',\\n        datefmt=\\'%Y-%m-%d %H:%M:%S\\',\\n        handlers=[\\n            logging.FileHandler(filename=LOGFILE),\\n            logging.StreamHandler(sys.stdout)\\n        ]\\n    )\\n    logger = logging.getLogger()\\n    return logger', 'configure_logger': <function configure_logger at 0x74ce3616a4d0>, '_i22': '# MAIN\\n\\n# Start of constants #############################################################################\\n\\n# path to dataset in new YOLO format \\nORIGINAL_DS_PATH = \\'/home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks\\'\\n\\n# path to latest weights file\\nORIGINAL_MODEL_PATH = f\\'{ORIGINAL_DS_PATH}/runs/detect/train5/weights/best.pt\\'\\n\\n# path to dataset in YOLOv5 format\\nNEW_DS_PATH = \\'/home/aubrey/datasets/Guam07v3\\'\\n\\n# name of FiftyOne dataset\\nFO_DATASET_NAME = \\'Guam07v3\\'\\n\\n# file name for log file saved in the same folder as this notebook\\nLOGFILE = \\'create_new_dataset.log\\'\\n\\n# Arguments for create_autocorrelated_images_view function.\\nAUTOCORRELATED_IMAGES_THRESHOLD = 0.98\\nDELETE_AUTOCORRELATED_IMAGES = True\\n\\n# Argument for create_autocorrelated_images_view function\\nDELETE_BBS_TOUCHING_EDGES = True\\n\\n# Option to retrain model. Usually FALSE.\\nRETRAIN_MODEL = False\\n\\n# Option to launch FiftyOne in browser at end of workflow. Usually True.\\nLAUNCH_51 = True\\n\\n# End of constants ########################################################################\\n\\n#configure logger\\nlogger = configure_logger(LOGFILE)\\n\\nlogger.info(globals())\\n\\n# update requirements.txt\\nlogger.info(\\'Updating \"requirements.txt\"\\')\\nupdate_requirements_file()\\n\\n# wrangle dataset into YOLOv5 format\\nif os.path.exists(NEW_DS_PATH):\\n    logger.info(f\\'\"{NEW_DS_PATH}\" already exists in YOLOv5 format\\')\\nelse:\\n    logger.info(f\\'creating dataset \"{NEW_DS_PATH}\" in YOLOv5 format\\')\\n    create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH)\\n\\n# Create new FiftyOne dataset\\nif FO_DATASET_NAME in fo.list_datasets():\\n    logger.info(f\\'FiftyOne dataset \"{FO_DATASET_NAME}\" already exists\\') \\nelse:\\n    logger.info(f\\'Creating FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\n    dataset = yolo2fiftyone(name=FO_DATASET_NAME, dataset_dir=NEW_DS_PATH)\\n    \\n# Load dataset\\nlogger.info(f\\'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\ndataset = fo.load_dataset(FO_DATASET_NAME)\\nlogger.info(f\\'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}\\')\\n\\n# Add fields if they don\\'t already exist\\nadd_field(\\'timestamp\\', add_timestamp_field)\\nadd_field(\\'embeddings\\', add_embeddings_field)\\nadd_field(\\'similarity_with_prev_img\\', add_similarity_with_prev_img_field)\\nadd_field(\\'yolov8\\', add_predictions_field)\\nadd_field(\\'mistakenness\\', add_mistakenness_field)\\n\\n# Find bounding boxes touching left, top or right edges of images\\nif \\'bb_touching_edge\\' in dataset.list_saved_views():\\n    logger.info(\\'\"bb_touching_edge_view\" already exists\\')\\nelse:\\n    logger.info(\\'Creating \"bb_touching_edge_view\"\\')\\n    if DELETE_BBS_TOUCHING_EDGES:\\n        logger.info(\\'    \"DELETE_BBS_TOUCHING_EDGES\" is True; bbs will be deleted\\')\\n    else:\\n        logger.info(\\'    \"DELETE_BBS_TOUCHING_EDGES\" is False; bbs will not be deleted\\')\\n    bb_touching_edge_count = create_bb_touching_edge_view(DELETE_BBS_TOUCHING_EDGES)\\n    logger.info(f\\'    {bb_touching_edge_count} ground truth bounding boxes touching image edges were found\\')\\n\\n# Find autocorrelated images\\nif \\'autocorrelated_images_view\\' in dataset.list_saved_views():\\n    logger.info(\\'\"autocorrelated_images_view\" already exists\\')\\nelse:\\n    logger.info(\\'Creating \"autocorrelated_images_view\"\\')\\n    if DELETE_BBS_TOUCHING_EDGES:\\n        logger.info(\\'    \"DELETE_AUTOCORRELATED_IMAGES\" is True; samples will be deleted\\')\\n    else:\\n        logger.info(\\'    \"DELETE_AUTOCORRELATED_IMAGES\" is False; bbs will not be deleted\\')\\n    autocorrelated_image_count = create_autocorrelated_images_view(\\n        threshold=AUTOCORRELATED_IMAGES_THRESHOLD, delete=DELETE_AUTOCORRELATED_IMAGES)\\n    logger.info(f\\'    With a threshold of {AUTOCORRELATED_IMAGES_THRESHOLD}, {autocorrelated_image_count} autocorrelated images were found\\')\\n\\nif RETRAIN_MODEL:\\n    export_51_to_YOLO(\\n        dataset_name=\\'Guam07v3\\', \\n        export_dir=\\'/home/aubrey/myexport\\', \\n        remove_unannotated=True)\\n    train_model()\\n\\nif LAUNCH_51:\\n    \\n    # Reload dataset\\n    logger.info(f\\'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"\\')\\n    dataset = fo.load_dataset(FO_DATASET_NAME)\\n    logger.info(f\\'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}\\')\\n\\n    # Launch FiftyOne app in browser\\n    logger.info(f\\'Launching FifyOne app in browser\\')\\n    session = fo.launch_app(dataset, auto=False)\\n    logger.info(session)\\n\\nlogger.info(\\'FINISHED\\')', 'ORIGINAL_DS_PATH': '/home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks', 'ORIGINAL_MODEL_PATH': '/home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks/runs/detect/train5/weights/best.pt', 'NEW_DS_PATH': '/home/aubrey/datasets/Guam07v3', 'FO_DATASET_NAME': 'Guam07v3', 'LOGFILE': 'create_new_dataset.log', 'AUTOCORRELATED_IMAGES_THRESHOLD': 0.98, 'DELETE_AUTOCORRELATED_IMAGES': True, 'DELETE_BBS_TOUCHING_EDGES': True, 'RETRAIN_MODEL': False, 'LAUNCH_51': True, 'logger': <RootLogger root (INFO)>}\n",
      "2024-11-27 17:41:07 Updating \"requirements.txt\"\n",
      "2024-11-27 17:41:07 \"/home/aubrey/datasets/Guam07v3\" already exists in YOLOv5 format\n",
      "2024-11-27 17:41:09 FiftyOne dataset \"Guam07v3\" already exists\n",
      "2024-11-27 17:41:09 Loading FiftyOne dataset \"Guam07v3\"\n",
      "2024-11-27 17:41:16     Ground truth bounding boxes: 14807\n",
      "2024-11-27 17:41:16 \"timestamp\" field already exists\n",
      "2024-11-27 17:41:16 \"embeddings\" field already exists\n",
      "2024-11-27 17:41:16 \"similarity_with_prev_img\" field already exists\n",
      "2024-11-27 17:41:16 \"yolov8\" field already exists\n",
      "2024-11-27 17:41:16 \"mistakenness\" field already exists\n",
      "2024-11-27 17:41:16 \"bb_touching_edge_view\" already exists\n",
      "2024-11-27 17:41:16 \"autocorrelated_images_view\" already exists\n",
      "2024-11-27 17:41:16 Loading FiftyOne dataset \"Guam07v3\"\n",
      "2024-11-27 17:41:23     Ground truth bounding boxes: 14807\n",
      "2024-11-27 17:41:23 Launching FifyOne app in browser\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "2024-11-27 17:41:25 Session launched. Run `session.show()` to open the App in a cell output.\n",
      "2024-11-27 17:41:25 Dataset:          Guam07v3\n",
      "Media type:       image\n",
      "Num samples:      8959\n",
      "Selected samples: 0\n",
      "Selected labels:  0\n",
      "Session URL:      http://localhost:5151/\n",
      "2024-11-27 17:41:25 FINISHED\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# Start of constants #############################################################################\n",
    "\n",
    "# path to dataset in new YOLO format \n",
    "ORIGINAL_DS_PATH = '/home/aubrey/Desktop/Guam07-training-set/datasets/3class-no-symlinks'\n",
    "\n",
    "# path to latest weights file\n",
    "ORIGINAL_MODEL_PATH = f'{ORIGINAL_DS_PATH}/runs/detect/train5/weights/best.pt'\n",
    "\n",
    "# path to dataset in YOLOv5 format\n",
    "NEW_DS_PATH = '/home/aubrey/datasets/Guam07v3'\n",
    "\n",
    "# name of FiftyOne dataset\n",
    "FO_DATASET_NAME = 'Guam07v3'\n",
    "\n",
    "# file name for log file saved in the same folder as this notebook\n",
    "LOGFILE = 'create_new_dataset.log'\n",
    "\n",
    "# Arguments for create_autocorrelated_images_view function.\n",
    "AUTOCORRELATED_IMAGES_THRESHOLD = 0.98\n",
    "DELETE_AUTOCORRELATED_IMAGES = True\n",
    "\n",
    "# Argument for create_autocorrelated_images_view function\n",
    "DELETE_BBS_TOUCHING_EDGES = True\n",
    "\n",
    "# Option to retrain model. Usually FALSE.\n",
    "RETRAIN_MODEL = False\n",
    "\n",
    "# Option to launch FiftyOne in browser at end of workflow. Usually True.\n",
    "LAUNCH_51 = True\n",
    "\n",
    "# End of constants ########################################################################\n",
    "\n",
    "#configure logger\n",
    "logger = configure_logger(LOGFILE)\n",
    "\n",
    "logger.info(globals())\n",
    "\n",
    "# update requirements.txt\n",
    "logger.info('Updating \"requirements.txt\"')\n",
    "update_requirements_file()\n",
    "\n",
    "# wrangle dataset into YOLOv5 format\n",
    "if os.path.exists(NEW_DS_PATH):\n",
    "    logger.info(f'\"{NEW_DS_PATH}\" already exists in YOLOv5 format')\n",
    "else:\n",
    "    logger.info(f'creating dataset \"{NEW_DS_PATH}\" in YOLOv5 format')\n",
    "    create_new_dataset(ORIGINAL_DS_PATH, NEW_DS_PATH)\n",
    "\n",
    "# Create new FiftyOne dataset\n",
    "if FO_DATASET_NAME in fo.list_datasets():\n",
    "    logger.info(f'FiftyOne dataset \"{FO_DATASET_NAME}\" already exists') \n",
    "else:\n",
    "    logger.info(f'Creating FiftyOne dataset \"{FO_DATASET_NAME}\"')\n",
    "    dataset = yolo2fiftyone(name=FO_DATASET_NAME, dataset_dir=NEW_DS_PATH)\n",
    "    \n",
    "# Load dataset\n",
    "logger.info(f'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"')\n",
    "dataset = fo.load_dataset(FO_DATASET_NAME)\n",
    "logger.info(f'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}')\n",
    "\n",
    "# Add fields if they don't already exist\n",
    "add_field('timestamp', add_timestamp_field)\n",
    "add_field('embeddings', add_embeddings_field)\n",
    "add_field('similarity_with_prev_img', add_similarity_with_prev_img_field)\n",
    "add_field('yolov8', add_predictions_field)\n",
    "add_field('mistakenness', add_mistakenness_field)\n",
    "\n",
    "# Find bounding boxes touching left, top or right edges of images\n",
    "if 'bb_touching_edge' in dataset.list_saved_views():\n",
    "    logger.info('\"bb_touching_edge_view\" already exists')\n",
    "else:\n",
    "    logger.info('Creating \"bb_touching_edge_view\"')\n",
    "    if DELETE_BBS_TOUCHING_EDGES:\n",
    "        logger.info('    \"DELETE_BBS_TOUCHING_EDGES\" is True; bbs will be deleted')\n",
    "    else:\n",
    "        logger.info('    \"DELETE_BBS_TOUCHING_EDGES\" is False; bbs will not be deleted')\n",
    "    bb_touching_edge_count = create_bb_touching_edge_view(DELETE_BBS_TOUCHING_EDGES)\n",
    "    logger.info(f'    {bb_touching_edge_count} ground truth bounding boxes touching image edges were found')\n",
    "\n",
    "# Find autocorrelated images\n",
    "if 'autocorrelated_images_view' in dataset.list_saved_views():\n",
    "    logger.info('\"autocorrelated_images_view\" already exists')\n",
    "else:\n",
    "    logger.info('Creating \"autocorrelated_images_view\"')\n",
    "    if DELETE_BBS_TOUCHING_EDGES:\n",
    "        logger.info('    \"DELETE_AUTOCORRELATED_IMAGES\" is True; samples will be deleted')\n",
    "    else:\n",
    "        logger.info('    \"DELETE_AUTOCORRELATED_IMAGES\" is False; bbs will not be deleted')\n",
    "    autocorrelated_image_count = create_autocorrelated_images_view(\n",
    "        threshold=AUTOCORRELATED_IMAGES_THRESHOLD, delete=DELETE_AUTOCORRELATED_IMAGES)\n",
    "    logger.info(f'    With a threshold of {AUTOCORRELATED_IMAGES_THRESHOLD}, {autocorrelated_image_count} autocorrelated images were found')\n",
    "\n",
    "if RETRAIN_MODEL:\n",
    "    export_51_to_YOLO(\n",
    "        dataset_name='Guam07v3', \n",
    "        export_dir='/home/aubrey/myexport', \n",
    "        remove_unannotated=True)\n",
    "    train_model()\n",
    "\n",
    "if LAUNCH_51:\n",
    "    \n",
    "    # Reload dataset\n",
    "    logger.info(f'Loading FiftyOne dataset \"{FO_DATASET_NAME}\"')\n",
    "    dataset = fo.load_dataset(FO_DATASET_NAME)\n",
    "    logger.info(f'    Ground truth bounding boxes: {count_ground_truth_bbs(dataset)}')\n",
    "\n",
    "    # Launch FiftyOne app in browser\n",
    "    logger.info(f'Launching FifyOne app in browser')\n",
    "    session = fo.launch_app(dataset, auto=False)\n",
    "    logger.info(session)\n",
    "\n",
    "logger.info('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.info['notes'] = 'Here is another note about this dataset.' \n",
    "# dataset.save()\n",
    "# dataset.info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
